{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rkrcgOoGafa"
      },
      "source": [
        "# Logistic Regression Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yB80VryGchN"
      },
      "source": [
        "#Q1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Logistic Regression is a classification algorithm used to predict binary outcomes (like Yes/No or 0/1). It models the probability that an instance belongs to a particular class.\n",
        "Linear Regression predicts continuous numerical values, whereas Logistic Regression predicts probabilities for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMI4Ac3OGcsl"
      },
      "source": [
        "#Q2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "P(Y=1|X) = 1/1+e^-(b0+b1x1+...bnxn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnKjsfbBGc67"
      },
      "source": [
        "#Q3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "The Sigmoid function maps predicted values to a range between 0 and 1, representing probabilities. It helps in interpreting the output as the probability of belonging to the positive class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AXrQmv8GdKQ"
      },
      "source": [
        "#Q4. What is the cost function of Logistic Regression?\n",
        "\n",
        "Logistic Regression uses Binary Cross-Entropy Loss (also called Log Loss):\n",
        "ùê∂ùëúùë†ùë°=‚àí1/ùëõ‚àë[ùë¶ùëñlog(ùë¶ùëñ^) + ( 1 ‚àí ùëñ ) log ‚Å° ( 1 ‚àí ùë¶ ùëñ ^ ) ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEEu4BeOGdWP"
      },
      "source": [
        "#Q5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Regularization adds a penalty term to the cost function to prevent overfitting by discouraging overly complex models.\n",
        "It keeps model coefficients small and improves generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpAXSj2MGdge"
      },
      "source": [
        "#Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
        "\n",
        "\n",
        "Lasso (L1): Can shrink some coefficients to exactly zero (feature selection).\n",
        "\n",
        "Ridge (L2): Shrinks coefficients but doesn‚Äôt set any to zero.\n",
        "\n",
        "Elastic Net: Combination of L1 and L2, balancing between shrinkage and feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK8OY1CtGdqn"
      },
      "source": [
        "#Q7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Use Elastic Net when there are many correlated features because it balances both Lasso (feature selection) and Ridge (coefficient shrinkage)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Panw1W9tGdzp"
      },
      "source": [
        "#Q8. What is the impact of the regularization parameter (Œª) in Logistic Regression?\n",
        "\n",
        "\n",
        "Higher Œª ‚Üí More regularization ‚Üí Simpler model, may underfit.\n",
        "\n",
        "Lower Œª ‚Üí Less regularization ‚Üí More complex model, may overfit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjJFv-dEGd-Y"
      },
      "source": [
        "#Q9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "\n",
        "No multicollinearity among independent variables\n",
        "\n",
        "Large sample size\n",
        "\n",
        "Linearity between independent variables and log-odds\n",
        "\n",
        "Independent observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mCU-MSgGeH_"
      },
      "source": [
        "#Q10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "\n",
        "Decision Trees\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "k-Nearest Neighbors (k-NN)\n",
        "\n",
        "Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niz_BnniGeSc"
      },
      "source": [
        "#Q11. What are Classification Evaluation Metrics?\n",
        "\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1-Score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz9DOcM5Gebf"
      },
      "source": [
        "#Q12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "When classes are imbalanced, the model may be biased towards the majority class, leading to poor performance on minority classes.\n",
        "Solutions include resampling techniques or using class weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwXx5l9FGeld"
      },
      "source": [
        "#Q13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Hyperparameter tuning involves finding the best set of parameters (like C, penalty, solver) that optimize model performance using techniques like GridSearchCV or RandomizedSearchCV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUXg3IsMGevF"
      },
      "source": [
        "#Q14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "\n",
        "liblinear: Good for small datasets, supports L1 and L2 regularization.\n",
        "\n",
        "lbfgs: Good for multiclass and large datasets.\n",
        "\n",
        "saga: Handles L1, L2, Elastic Net; good for large datasets.\n",
        "Choose solver based on dataset size and regularization needs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ildSsrcaGe4t"
      },
      "source": [
        "#Q15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "\n",
        "One-vs-Rest (OvR): Build one binary classifier per class.\n",
        "\n",
        "Softmax Regression: Generalization for multiple classes at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmSC8NIeGfB9"
      },
      "source": [
        "#Q16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and easy to implement\n",
        "\n",
        "Outputs probabilities\n",
        "\n",
        "Works well with linearly separable data\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Struggles with non-linear relationships\n",
        "\n",
        "Assumes linear decision boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtmgElgAGfKm"
      },
      "source": [
        "#Q17. What are some use cases of Logistic Regression?\n",
        "\n",
        "\n",
        "Spam email detection\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Disease diagnosis (yes/no)\n",
        "\n",
        "Credit card fraud detection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVnJ2_iOGfUE"
      },
      "source": [
        "#Q18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Logistic Regression handles binary classification.\n",
        "\n",
        "Softmax Regression handles multiclass classification by assigning probabilities across multiple classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzgTFM5ZGfdV"
      },
      "source": [
        "#Q19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "Use OvR for simple, smaller datasets where binary classifiers are efficient.\n",
        "\n",
        "Use Softmax for large, complex multiclass problems needing probability distribution across classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgSV365HGfmj"
      },
      "source": [
        "#Q20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "The coefficients represent the change in the log-odds of the outcome for a one-unit increase in the predictor variable, holding other variables constant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "344lt8JqGfxH"
      },
      "source": [
        "# Logistic Regression Practical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkz5Q5AuI1xy",
        "outputId": "71ec5834-dd87-4497-9f70-9027b36b07da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 1. Write a Python program that loads a dataset, splits it into training and testing sets,\n",
        "# applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV7msKitI1oI",
        "outputId": "86ee88b2-555c-4e1e-e470-e6a010144864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L1 Regularized Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 2. Write a Python program to apply L1 regularization (Lasso) on a dataset\n",
        "# using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "model_l1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model_l1.fit(X_train, y_train)\n",
        "y_pred_l1 = model_l1.predict(X_test)\n",
        "\n",
        "print(\"L1 Regularized Accuracy:\", accuracy_score(y_test, y_pred_l1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxzP-ejeI1ec",
        "outputId": "d7105b87-1c47-4f3d-a3c8-2ca981d76057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L2 Regularized Accuracy: 1.0\n",
            "Coefficients: [[-0.43107698  0.84570847 -2.15658006 -0.88940818]]\n"
          ]
        }
      ],
      "source": [
        "# 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge)\n",
        "# using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "model_l2 = LogisticRegression(penalty='l2')\n",
        "model_l2.fit(X_train, y_train)\n",
        "y_pred_l2 = model_l2.predict(X_test)\n",
        "\n",
        "print(\"L2 Regularized Accuracy:\", accuracy_score(y_test, y_pred_l2))\n",
        "print(\"Coefficients:\", model_l2.coef_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p10rPjhRI1U3",
        "outputId": "1de7f737-7d07-418c-a09c-139c5807f10d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elastic Net Accuracy: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Gaurav Rai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "model_elastic = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model_elastic.fit(X_train, y_train)\n",
        "\n",
        "y_pred_elastic = model_elastic.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_elastic)\n",
        "print(\"Elastic Net Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXvW9VOdI1NB",
        "outputId": "9fe25f4d-24e5-453e-b70c-013102df0c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multiclass (OvR) Accuracy: 0.9555555555555556\n"
          ]
        }
      ],
      "source": [
        "# 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model_multiclass = OneVsRestClassifier(LogisticRegression())\n",
        "model_multiclass.fit(X_train, y_train)\n",
        "\n",
        "y_pred_multiclass = model_multiclass.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_multiclass)\n",
        "print(\"Multiclass (OvR) Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JroSCT18I1Cx",
        "outputId": "53f3aa48-0887-4e49-aceb-6f68b59c59fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy on Validation: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression.\n",
        "# Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy on Validation:\", grid.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d5deJbaI057",
        "outputId": "31af165b-6932-4f9a-8a22-7967d81f0f1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Scores: [1. 1. 1. 1. 1.]\n",
            "Average Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation.\n",
        "# Print the average accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "model_skf = LogisticRegression()\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "scores = cross_val_score(model_skf, X, y_binary, cv=skf)\n",
        "\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajD5LQYKI0w7",
        "outputId": "ab5d27c3-7830-4c43-e0c5-be481465592c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV Dataset Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 8. Write a Python program to load a dataset from a CSV file,\n",
        "# apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Simulating a CSV using Iris data\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = (data.target == 0).astype(int)\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"CSV Dataset Accuracy:\", model.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UolxWrBLKGgI",
        "outputId": "3a4b2b3c-1f0b-4f69-a66e-be82b3cfc264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'C': 3.7554011884736247, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy on Validation: 1.0\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver)\n",
        "# in Logistic Regression. Print the best parameters and accuracy.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import scipy.stats as stats\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import scipy.stats as stats\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "param_dist = {\n",
        "    'C': stats.uniform(0.01, 10),  \n",
        "    'penalty': ['l1', 'l2'],        \n",
        "    'solver': ['liblinear', 'saga'] ]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=1000), param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy on Validation:\", random_search.best_score_)\n",
        "\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7mIn-JEI0nh",
        "outputId": "5abdf50d-da2c-4a42-b854-0ffb86cdf4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-vs-One (OvO) Multiclass Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression\n",
        "# and print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression())\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ovo = ovo_model.predict(X_test)\n",
        "\n",
        "print(\"One-vs-One (OvO) Multiclass Accuracy:\", accuracy_score(y_test, y_pred_ovo))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "dVicyr1mI0fi",
        "outputId": "63489abe-d064-46bb-8187-b8942c29085e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-riXpdYsI0We",
        "outputId": "d43e651f-839d-4c1c-9bca-ada3a509fe03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsUcshFDI0N4",
        "outputId": "8dff1688-1117-4bac-8148-d0f5f3e7e8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with class weights: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1], n_informative=3, n_redundant=0, flip_y=0, n_features=5, n_clusters_per_class=1, n_samples=1000, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model_weighted = LogisticRegression(class_weight='balanced')\n",
        "model_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "\n",
        "print(\"Accuracy with class weights:\", accuracy_score(y_test, y_pred_weighted))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxU7DSARI0De",
        "outputId": "6ff5ac2a-a88b-4814-d497-8e504171d84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Titanic Dataset Accuracy: 0.8097014925373134\n"
          ]
        }
      ],
      "source": [
        "# 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Feature selection and preprocessing\n",
        "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "df = df.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1)\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Titanic Dataset Accuracy:\", model.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oSUtfQPIz6o",
        "outputId": "32e318cc-5c92-4802-f0c5-208230664517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy without scaling: 0.8097014925373134\n",
            "Accuracy with scaling: 0.8097014925373134\n"
          ]
        }
      ],
      "source": [
        "# 15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model.\n",
        "# Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "acc_no_scaling = model_no_scaling.score(X_test, y_test)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaling = LogisticRegression(max_iter=1000)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "acc_scaling = model_scaling.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with scaling:\", acc_scaling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZRd2t5NIzy4",
        "outputId": "8844e990-aa10-437d-9691-4e1a62dc85e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC-AUC Score: 0.8804728295174155\n"
          ]
        }
      ],
      "source": [
        "# 16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkM9OWdoIzqI",
        "outputId": "49ade761-44fc-427d-ab98-e0cfdc4803db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with C=0.5: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model_cust = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model_cust.fit(X_train, y_train)\n",
        "\n",
        "accuracy = model_cust.score(X_test, y_test)\n",
        "print(\"Accuracy with C=0.5:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrJJsbHuIzhk",
        "outputId": "e4d43a92-ccf0-47c4-83af-9752463be603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Importance:\n",
            "sepal width (cm)     0.966541\n",
            "sepal length (cm)   -0.821750\n",
            "petal width (cm)    -1.241644\n",
            "petal length (cm)   -1.358033\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# 18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "feature_names = data.feature_names\n",
        "X = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model_cust = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model_cust.fit(X_train, y_train)\n",
        "\n",
        "feature_importance = pd.Series(model_cust.coef_[0], index=X.columns)\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance.sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZoqABLiCIzZV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cohen's Kappa Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa Score.\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "y_pred_cust = model_cust.predict(X_test)\n",
        "kappa = cohen_kappa_score(y_test, y_pred_cust)\n",
        "\n",
        "print(\"Cohen's Kappa Score:\", kappa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "t5fZgM2tIzQZ",
        "outputId": "59872908-433a-4ccc-e028-2de69b4deca4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "feature_names = data.feature_names\n",
        "X = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model_cust = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model_cust.fit(X_train, y_train)\n",
        "\n",
        "y_pred_prob = model_cust.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d7xNUgkIzI5",
        "outputId": "2988ab2a-0f81-44ab-c74d-929e7a5d6407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solver: liblinear -> Accuracy: 1.0\n",
            "Solver: saga -> Accuracy: 1.0\n",
            "Solver: lbfgs -> Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "for solver in solvers:\n",
        "    model_solver = LogisticRegression(solver=solver, max_iter=5000)\n",
        "    model_solver.fit(X_train, y_train)\n",
        "    acc_solver = model_solver.score(X_test, y_test)\n",
        "    print(f\"Solver: {solver} -> Accuracy: {acc_solver}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOdGzTLsIzAF",
        "outputId": "287481ed-e982-47b3-ab67-1dbe867b5fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matthews Correlation Coefficient (MCC): 1.0\n"
          ]
        }
      ],
      "source": [
        "# 22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "feature_names = data.feature_names\n",
        "X = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_solver = model.predict(X_test)\n",
        "mcc = matthews_corrcoef(y_test, y_pred_solver)\n",
        "\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoDlPEc-Iy3D",
        "outputId": "599dda33-a81a-41d8-90c0-73d237202772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on Raw Data: 1.0\n",
            "Accuracy on Standardized Data: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 23. Write a Python program to train Logistic Regression on both raw and standardized data.\n",
        "# Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "# (Already covered earlier in scaling question - here summarize)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "feature_names = data.feature_names\n",
        "X = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train on raw data\n",
        "model_raw = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train on standardized data\n",
        "model_scaled = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy on Raw Data:\", accuracy_raw)\n",
        "print(\"Accuracy on Standardized Data:\", accuracy_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ligPYIR5Iysq",
        "outputId": "6459ba43-b8a3-4cee-b0a8-2035612502a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best C value: {'C': 0.1}\n"
          ]
        }
      ],
      "source": [
        "# 24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid_c = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "grid_c = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_c, cv=5)\n",
        "grid_c.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best C value:\", grid_c.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyYnU4k3Iyje",
        "outputId": "78e2092d-ab91-45b3-cd93-32fd7ed8d652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded Model Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Train and save\n",
        "model_save = LogisticRegression(max_iter=1000)\n",
        "model_save.fit(X_train, y_train)\n",
        "joblib.dump(model_save, 'logistic_model.pkl')\n",
        "\n",
        "# Load and predict\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "print(\"Loaded Model Accuracy:\", loaded_model.score(X_test, y_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7rkrcgOoGafa"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
